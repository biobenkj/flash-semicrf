% Streaming Semi-CRF Algorithm Supplement
% For inclusion in manuscript supplemental material
%
% Required packages:
% \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% \usepackage{amsmath,amssymb}

\documentclass{article}
\usepackage[margin=1.25in]{geometry}  % Increased margins
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}

\section*{Supplemental Material: Streaming Semi-CRF Inference}

This supplement provides formal algorithmic details for the streaming Semi-CRF implementation.

%==============================================================================
\section{Notation}
%==============================================================================

\begin{table}[h]
\centering
\begin{tabular}{c l l}
\hline
\textbf{Symbol} & \textbf{Description} & \textbf{Shape} \\
\hline
$T$ & Sequence length & scalar \\
$K$ & Maximum segment duration & scalar \\
$C$ & Number of labels (states) & scalar \\
$B$ & Batch size & scalar \\
$\mathcal{S}_{t,c}$ & Cumulative projected scores & $(B, T+1, C)$ \\
$\mathcal{T}_{c',c}$ & Transition scores (source $c'$ to dest $c$) & $(C, C)$ \\
$\mathcal{B}_{k,c}$ & Duration bias for duration $k$, label $c$ & $(K, C)$ \\
$\tilde{\alpha}_t(c)$ & Log-forward message at position $t$, label $c$ & $(B, C)$ \\
$\tilde{\beta}_t(c)$ & Log-backward message & $(B, C)$ \\
$\boldsymbol{\alpha}$ & Ring buffer for forward messages & $(B, K, C)$ \\
$\Omega$ & Checkpointed ring buffer states & $(B, N, K, C)$ \\
$\mathcal{N}$ & Cumulative log-normalization factors & $(B, N)$ \\
$\Delta$ & Checkpoint interval ($\approx \sqrt{TK}$) & scalar \\
$\log Z$ & Log partition function & $(B,)$ \\
\hline
\end{tabular}
\caption{Notation. Tilde ($\tilde{\cdot}$) denotes log-domain quantities. $N = \lceil T / \Delta \rceil$ is the number of checkpoints.}
\label{tab:notation}
\end{table}

%==============================================================================
\section{Edge Potential Decomposition}
%==============================================================================

The key innovation enabling $O(KC)$ memory is computing edge potentials on-the-fly from cumulative scores:

\begin{equation}
\label{eq:streaming_potential}
\tilde{\psi}(t, k, c, c') =
\underbrace{\left(\mathcal{S}_{t, c} - \mathcal{S}_{t-k, c}\right)}_{\text{segment content}} +
\underbrace{\mathcal{B}_{k, c}}_{\text{duration bias}} +
\underbrace{\mathcal{T}_{c', c}}_{\text{transition}}
\end{equation}

\noindent where $t$ is the segment end position (1-indexed), $k \in \{1, \ldots, K\}$ is the duration, $c$ is the destination state, and $c'$ is the source state. The prefix-sum decomposition allows $O(1)$ edge computation instead of $O(k)$.

%==============================================================================
\section{Streaming Forward Algorithm}
%==============================================================================

Algorithm~\ref{alg:streaming_forward} maintains a ring buffer $\boldsymbol{\alpha} \in \mathbb{R}^{K \times C}$ storing the $K$ most recent forward messages. Following the Flash Attention~\cite{dao2022flashattention} and Mamba~\cite{gu2023mamba} pattern, we normalize at checkpoint boundaries to prevent overflow at extreme $T$.

\begin{algorithm}[t]
\caption{Streaming Semi-CRF Forward Scan}\label{alg:streaming_forward}
\KwIn{$\mathcal{S}$, $\mathcal{T}$, $\mathcal{B}$, checkpoint interval $\Delta$}
\KwOut{$\log Z$, checkpoints $(\Omega, \mathcal{N})$}
\BlankLine
$\boldsymbol{\alpha} \gets -\infty$; \quad $\boldsymbol{\alpha}[0, :] \gets 0$; \quad $\mathcal{N}_{\text{accum}} \gets 0$\;
$\Omega[0] \gets \boldsymbol{\alpha}$; \quad $\mathcal{N}[0] \gets 0$\;
\BlankLine
\For{$t \gets 1$ \KwTo $T$}{
    $\mathbf{v}_t \gets -\infty \in \mathbb{R}^C$\;
    \For{$k \gets 1$ \KwTo $\min(K, t)$}{
        $\tilde{\boldsymbol{\alpha}}_{\text{prev}} \gets \boldsymbol{\alpha}[(t-k) \bmod K, :]$\;
        $\mathbf{h} \gets \bigl(\mathcal{S}_{t,:} - \mathcal{S}_{t-k,:}\bigr) + \mathcal{B}_{k,:}$\;
        $\mathbf{E} \gets \mathbf{h}[:, \text{None}] + \mathcal{T}^\top$\;
        $\mathbf{s}_k \gets \text{LSE}(\tilde{\boldsymbol{\alpha}}_{\text{prev}}[\text{None}, :] + \mathbf{E}, \text{axis}=1)$\;
        $\mathbf{v}_t \gets \text{LSE}(\mathbf{v}_t, \mathbf{s}_k)$\;
    }
    $\boldsymbol{\alpha}[t \bmod K, :] \gets \mathbf{v}_t$\;
    \BlankLine
    \If(\tcp*[f]{Normalize at checkpoint~\cite{dao2022flashattention}}){$t \bmod \Delta = 0$}{
        $n \gets t / \Delta$; \quad $s \gets \max_c \mathbf{v}_t(c)$\;
        $\mathcal{N}_{\text{accum}} \gets \mathcal{N}_{\text{accum}} + s$; \quad $\boldsymbol{\alpha} \gets \boldsymbol{\alpha} - s$\;
        $\Omega[n] \gets \boldsymbol{\alpha}$; \quad $\mathcal{N}[n] \gets \mathcal{N}_{\text{accum}}$\;
    }
}
\BlankLine
$\log Z \gets \text{LSE}(\boldsymbol{\alpha}[T \bmod K, :]) + \mathcal{N}_{\text{accum}}$\;
\Return{$\log Z$, $(\Omega, \mathcal{N})$}\;
\end{algorithm}

\textbf{Memory}: $O(KC)$ ring buffer + $O(\sqrt{T/K} \cdot KC)$ checkpoints. \textbf{Time}: $O(TKC^2)$.

%==============================================================================
\section{Backward Pass with Checkpointing}
%==============================================================================

Algorithm~\ref{alg:streaming_backward} processes segments in reverse, recomputing $\alpha$ from checkpoints following gradient checkpointing~\cite{chen2016training}. The saved $\mathcal{N}_i$ restores the true scale when computing marginals.

\begin{algorithm}[t]
\caption{Streaming Semi-CRF Backward Pass}\label{alg:streaming_backward}
\KwIn{$\mathcal{S}$, $\mathcal{T}$, $\mathcal{B}$, checkpoints $(\Omega, \mathcal{N})$, $\log Z$, upstream $\partial \mathcal{L}/\partial Z$}
\KwOut{$\nabla \mathcal{S}$, $\nabla \mathcal{T}$, $\nabla \mathcal{B}$}
\BlankLine
$\boldsymbol{\beta} \gets -\infty$; \quad $\boldsymbol{\beta}[T \bmod K, :] \gets 0$\;
Initialize gradient accumulators\;
\BlankLine
\For{$i \gets N_{\text{ckpts}}-1$ \KwTo $0$}{
    $t_{\text{start}} \gets i \cdot \Delta$; \quad $t_{\text{end}} \gets \min((i+1) \cdot \Delta, T)$\;
    $\mathcal{N}_i \gets \mathcal{N}[i]$\;
    $\boldsymbol{\alpha}_{\text{local}} \gets$ \textsc{RecomputeAlpha}($\Omega[i]$, $t_{\text{start}}$, $t_{\text{end}}$)\;
    \BlankLine
    \For{$t \gets t_{\text{end}}-1$ \KwTo $t_{\text{start}}$}{
        $\tilde{\boldsymbol{\alpha}}_t \gets \boldsymbol{\alpha}_{\text{local}}[t - t_{\text{start}}, :]$\;
        $\tilde{\boldsymbol{\beta}}_t \gets -\infty$\;
        \BlankLine
        \For{$k \gets 1$ \KwTo $\min(K, T-t)$}{
            $\tilde{\boldsymbol{\beta}}_{\text{next}} \gets \boldsymbol{\beta}[(t+k) \bmod K, :]$\;
            $\tilde{\psi} \gets$ Eq.~\eqref{eq:streaming_potential}\;
            \BlankLine
            \tcp{Log-norm correction restores true $\alpha$ scale}
            $\log \mu \gets \tilde{\boldsymbol{\alpha}}_t[\text{None}, :] + \tilde{\psi} + \tilde{\boldsymbol{\beta}}_{\text{next}}[:, \text{None}] + \mathcal{N}_i - \log Z$\;
            $\mu \gets \exp(\log \mu)$\;
            \BlankLine
            Accumulate $\nabla \mathcal{S}$, $\nabla \mathcal{T}$, $\nabla \mathcal{B}$ from $\mu$\;
            $\tilde{\boldsymbol{\beta}}_t \gets \text{LSE}(\tilde{\boldsymbol{\beta}}_t, \text{LSE}(\tilde{\psi} + \tilde{\boldsymbol{\beta}}_{\text{next}}[:, \text{None}], \text{axis}=0))$\;
        }
        $\boldsymbol{\beta}[t \bmod K, :] \gets \tilde{\boldsymbol{\beta}}_t$\;
    }
}
\Return{$\nabla \mathcal{S}$, $\nabla \mathcal{T}$, $\nabla \mathcal{B}$}\;
\end{algorithm}

%==============================================================================
\section{Gradient Computation}
\label{sec:gradients}
%==============================================================================

Gradients are computed via marginal probabilities:

\begin{equation}
\mu(t, k, c, c') = \exp\bigl(\tilde{\alpha}_{t-k}(c') + \tilde{\psi}(t, k, c, c') + \tilde{\beta}_t(c) + \mathcal{N}_i - \log Z\bigr)
\end{equation}

For per-sequence parameters:
\begin{equation}
\nabla \mathcal{S}_{t,c} = \frac{\partial \mathcal{L}}{\partial Z_b} \cdot \left( \sum_{k, c'} \mu_b(t, k, c, c') - \sum_{k, c'} \mu_b(t+k, k, c, c') \right)
\end{equation}

For shared parameters, we accumulate per-batch then reduce via einsum:
\begin{align}
\nabla \mathcal{T}_{c',c} &= \sum_{b} \frac{\partial \mathcal{L}}{\partial Z_b} \cdot \sum_{t,k} \mu_b(t, k, c, c') \\
\nabla \mathcal{B}_{k,c} &= \sum_{b} \frac{\partial \mathcal{L}}{\partial Z_b} \cdot \sum_{t, c'} \mu_b(t, k, c, c')
\end{align}

%==============================================================================
\section{Numerical Stability}
%==============================================================================

\textbf{Zero-centering.} Cumulative scores are zero-centered before cumsum to bound magnitude at large $T$:
$\bar{s}_{t,c} = s_{t,c} - \frac{1}{T}\sum_{\tau} s_{\tau,c}$.

\textbf{Log-domain.} All computations use logsumexp: $\text{LSE}(\mathbf{x}) = \max(\mathbf{x}) + \log\sum_i \exp(x_i - \max(\mathbf{x}))$.

\textbf{Masking.} Invalid positions use $-10^9$ (not $-\infty$) to avoid NaN in gradients.

\textbf{Variable-length batches.} For sequences ending at $L < T$, the normalization shift is masked to zero for $t > L$, freezing $\mathcal{N}_{\text{accum}}$ at the correct value.

%==============================================================================
\section{Implementation Correspondence}
%==============================================================================

\begin{table}[h]
\centering
\begin{tabular}{l l l}
\hline
\textbf{Math} & \textbf{Code Variable} & \textbf{Notes} \\
\hline
$\mathcal{S}_{t,c}$ & \texttt{cum\_scores[:, t, c]} & Boundary at index 0 \\
$\mathcal{T}_{c',c}$ & \texttt{transition[c\_src, c\_dst]} & Source-first storage \\
$\mathcal{B}_{k,c}$ & \texttt{duration\_bias[k, c]} & $k \in \{1, \ldots, K\}$ \\
$\boldsymbol{\alpha}$ & \texttt{alpha\_ring} & Ring buffer \\
$\Omega$ & \texttt{ring\_checkpoints} & Saved at intervals \\
$\mathcal{N}$ & \texttt{log\_norm\_checkpoints} & Cumulative log-norm \\
$\Delta$ & \texttt{checkpoint\_interval} & $\approx \sqrt{TK}$ \\
\hline
\end{tabular}
\caption{Mapping between mathematical notation and implementation.}
\label{tab:implementation}
\end{table}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{dao2022flashattention}
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.
\emph{NeurIPS}, 2022.

\bibitem{gu2023mamba}
Albert Gu and Tri Dao.
Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
\emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
Training Deep Nets with Sublinear Memory Cost.
\emph{arXiv preprint arXiv:1604.06174}, 2016.

\end{thebibliography}

\end{document}